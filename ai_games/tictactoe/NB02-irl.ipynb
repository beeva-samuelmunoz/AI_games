{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 02 Inverse Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "\n",
    "\n",
    "### Nomenclature\n",
    "1. $S$: set of states.\n",
    "1. $A$: set of actions.\n",
    "1. $R$: set of rewards (per states).\n",
    "1. $\\pi$: optimal policy.\n",
    "1. $T^\\pi$: policy transition matrix of size $n√ón$ containing state transition probabilities for always choosing $\\pi(s)$ in each respective state.\n",
    " \n",
    "\n",
    "\n",
    "### Assumptions\n",
    "1. Finite, small state spaces.\n",
    "1. We know the expert's policy $\\pi$. \n",
    "1. $\\pi$ is optimal and stationary deterministic.\n",
    "\n",
    "\n",
    "### Linear problem\n",
    "Get the rewards $R$ for the states under an optimal policy $\\pi$.\n",
    "\n",
    "$$\n",
    "\\forall \\mathbf{T}^i \\in \\mathbf{T}^{\\neg\\pi}: (\\mathbf{T}^\\pi-\\mathbf{T}^i)(\\mathbf{I} - \\gamma\\mathbf{T}^\\pi)^{-1} \\mathbf{R} \\succeq 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the optimum policy Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.sparse import linalg\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "from board import Board\n",
    "from players.minimax import Minimax\n",
    "from players.qlearner import QLearner\n",
    "from utils import test_players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent =  QLearner(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t 152546 games played\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "TRAIN_SECONDS = 180\n",
    "\n",
    "games_played = 0\n",
    "time_start = time.time()\n",
    "time_stop = time.time() + TRAIN_SECONDS\n",
    "while time.time() < time_stop:\n",
    "    agent._autotrain_1_game(\n",
    "        lr=0.1, \n",
    "        f_df=lambda x: 1-(x/10),\n",
    "        board=Board()\n",
    "    )\n",
    "    games_played += 1\n",
    "\n",
    "print(\"\\t {} games played\\n\".format(games_played))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing agent VS minimax (100 games)\n",
      "\tAs player 1\n",
      "{1: 0, 2: 0, -1: 100}\n",
      "\tAs player 2\n",
      "{1: 0, 2: 0, -1: 100}\n"
     ]
    }
   ],
   "source": [
    "# Test the agent, should tie (-1) against an optimum player (i.e. minimax)\n",
    "minimax = Minimax(1)\n",
    "N_GAMES = 100\n",
    "\n",
    "print(\"Testing agent VS minimax ({} games)\".format(N_GAMES))\n",
    "print(\"\\tAs player 1\")\n",
    "print(test_players(agent, minimax, N_GAMES))\n",
    "print(\"\\tAs player 2\")\n",
    "print(test_players(agent, minimax, N_GAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear problem\n",
    "\n",
    "Original formula:\n",
    "$$\n",
    "\\forall \\mathbf{T}^i \\in \\mathbf{T}^{\\neg\\pi}: (\\mathbf{T}^\\pi-\\mathbf{T}^i)(\\mathbf{I} - \\gamma\\mathbf{T}^\\pi)^{-1} \\mathbf{R} \\succeq 0\n",
    "$$\n",
    "\n",
    "Formula adapted to solver:\n",
    "$$\n",
    "\\forall \\mathbf{T}^i \\in \\mathbf{T}^{\\neg\\pi}: (\\mathbf{T}^i-\\mathbf{T}^\\pi)(\\mathbf{I} - \\gamma\\mathbf{T}^\\pi)^{-1} \\mathbf{R} \\preceq 0\n",
    "$$\n",
    "\n",
    "*Notes:*\n",
    "* T_factor1 $= (\\mathbf{T}^i-\\mathbf{T}^\\pi)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = agent.Q\n",
    "\n",
    "T_pi = {  # Transition probabilities under optimal policy (always 1)\n",
    "    #'val': [], # Transition probability (1)\n",
    "    'row': [], # Origin state\n",
    "    'col': []  # Destination state\n",
    "}\n",
    "\n",
    "T_factor1 = {  # Precalculated inecuation (T^i - T^pi)\n",
    "    # T^i is 1, T^pi is -1, ultra-sparse matrix\n",
    "    'val': [],\n",
    "    'row': [],\n",
    "    'col': []\n",
    "}\n",
    "\n",
    "# Index state->position in matrix\n",
    "index_states = {state:state_i for state_i,state in enumerate(Q.keys())}\n",
    "\n",
    "def get_position(state):  # TODO, will be a method in a class\n",
    "    '''Get the position of state in the index.\n",
    "    If not in the index, append at the end.\n",
    "    '''\n",
    "    retval = index_states.get(state)  # Destination state\n",
    "    if not retval:  # If state not in Q (final state), add to index\n",
    "        retval = len(index_states)\n",
    "        index_states[state] = retval\n",
    "    return retval\n",
    "    \n",
    "\n",
    "try:\n",
    "    ineq_i = 0  # inequation index\n",
    "    for state,state_i in list(index_states.items()):  # Make it a list, dict will change on the run\n",
    "        # Populate T_pi, take the best action and add it.\n",
    "        actions = list(Q[state].items())   # [(action, score)]\n",
    "        best_action = max(actions, key=lambda x: x[1])\n",
    "        row,col = best_action[0] # Move in the board\n",
    "        board = Board(board=state)\n",
    "        board.move(row,col,1)  # Defensive programming, better execute action than creating m\n",
    "        #T_pi['val'].append(1)  # Transition is deterministic 1\n",
    "        T_pi['row'].append(state_i) # Orgin state\n",
    "        state_i_dest_opt = get_position(board.get_state())\n",
    "        T_pi['col'].append(state_i_dest_opt)\n",
    "\n",
    "        actions.remove(best_action)\n",
    "        # Calculate T_i lines for that state, non-optimal moves\n",
    "        for (row,col),score in actions:\n",
    "            board = Board(board=state)\n",
    "            board.move(row,col,1)\n",
    "            # Non-optimal move\n",
    "            T_factor1['row'].append(ineq_i)\n",
    "            T_factor1['col'].append(get_position(board.get_state()))\n",
    "            T_factor1['val'].append(1) # 1-0\n",
    "            # Optimal move\n",
    "            T_factor1['row'].append(ineq_i)\n",
    "            T_factor1['col'].append(state_i_dest_opt)\n",
    "            T_factor1['val'].append(-1) # 0-1    \n",
    "            ineq_i += 1 # Next inequation \n",
    "except Exception as e:\n",
    "    print(state)\n",
    "    print(actions)\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = len(index_states)\n",
    "\n",
    "M_factor1 = sparse.csr_matrix(\n",
    "    (T_factor1['val'], (T_factor1['row'], T_factor1['col'])),\n",
    "    shape=(max(T_factor1['row'])+1, n_states)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samuelmunoz/beeva/github/ml-ai_games/venv/lib/python3.5/site-packages/scipy/sparse/linalg/dsolve/linsolve.py:295: SparseEfficiencyWarning: splu requires CSC matrix format\n",
      "  warn('splu requires CSC matrix format', SparseEfficiencyWarning)\n",
      "/home/samuelmunoz/beeva/github/ml-ai_games/venv/lib/python3.5/site-packages/scipy/sparse/linalg/dsolve/linsolve.py:202: SparseEfficiencyWarning: spsolve is more efficient when sparse b is in the CSC matrix format\n",
      "  'is in the CSC matrix format', SparseEfficiencyWarning)\n"
     ]
    }
   ],
   "source": [
    "DISCOUNT_FACTOR = 0.1\n",
    "\n",
    "M_factor2 = linalg.inv(\n",
    "    sparse.identity(n_states) - \n",
    "    (DISCOUNT_FACTOR * sparse.csr_matrix( \n",
    "        (np.ones(len(T_pi['row'])), (T_pi['row'], T_pi['col'])),\n",
    "        shape=(n_states, n_states)\n",
    "    ))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_ineq = M_factor1.dot(M_factor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 7413\n",
      "Shape inequations matrix: \n",
      "(11623, 7413)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of states: \"+str(n_states))\n",
    "print(\"Shape inequations matrix: \")\n",
    "print(M_ineq.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: -0.0\n",
      " message: 'Optimization terminated successfully.'\n",
      "     nit: 0\n",
      "   slack: array([0., 0., 0., ..., 0., 0., 0.])\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0., 0., 0., ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "result = linprog(\n",
    "    c = np.ones(n_states),  # Function coefficients\n",
    "    A_ub = M_ineq.todense(),          # Constraint coefficients, this<=_\n",
    "    b_ub = np.zeros(M_ineq.shape[0])  # Constraint values _<=this \n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A degenerate solution\n",
    "\n",
    "The problem is I get a degenerate solution.\n",
    "\n",
    "Rewards equal to zero is a solution to the inequations. To filter this solution I will try to change the function to optimize.\n",
    "\n",
    "$$\n",
    "\\text{maximize} : \\sum_{s\\in\\mathcal{S}} (Q^\\pi(s, \\pi(s))-\\text{max}_{a\\in\\mathcal{A}\\setminus \\pi(s)}Q^\\pi(s, a))\n",
    "$$\n",
    "\n",
    "Maximize the difference between the first and second best solutions.\n",
    "\n",
    "Expressed as a minimization problem:\n",
    "$$\n",
    "\\text{minimize} : \\sum_{s\\in\\mathcal{S}} (Q^\\pi(s, \\pi(s))+\\text{max}_{a\\in\\mathcal{A}\\setminus \\pi(s)}Q^\\pi(s, a))\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeficients = np.zeros(n_states)  # How should I model final states? No transitions.\n",
    "\n",
    "for state,state_i in list(index_states.items()):  # Make it a list, dict will change on the run\n",
    "    # Populate T_pi, take the best action and add it.\n",
    "    actions = list(Q[state].items())   # [(action, score)]\n",
    "    if actions:\n",
    "        best_action = _,score1 = max(actions, key=lambda x: x[1])\n",
    "        actions.remove(best_action)\n",
    "        if actions:\n",
    "            _,score2 = max(actions, key=lambda x: x[1])\n",
    "        else:\n",
    "            score2 = 0\n",
    "        coeficients[state_i] = score1 + score2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upper bound for Rewards (coeficients)\n",
    "$$\n",
    "\\forall s \\in \\mathcal{S}: |\\hat{\\mathcal{R}}(s)| \\leq R_{\\text{max}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "M_ineq_coeffs = np.identity(n_states)\n",
    "R_max = 1 # No reward higher than 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = M_ineq.todense()\n",
    "Z = np.vstack([Z,M_ineq_coeffs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states: 7413\n",
      "Shape inequations matrix: \n",
      "(19036, 7413)\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of states: \"+str(n_states))\n",
    "print(\"Shape inequations matrix: \")\n",
    "print(Z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     fun: -312.19582386834895\n",
      " message: 'Optimization terminated successfully.'\n",
      "     nit: 481\n",
      "   slack: array([0., 0., 0., ..., 1., 1., 1.])\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([0., 0., 0., ..., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "result = linprog(\n",
    "    c = coeficients,  # Function coefficients, minimize this function\n",
    "    A_ub = Z,          # Constraint coefficients, this<=_\n",
    "    b_ub = np.array(\n",
    "        [0]*M_ineq.shape[0] +\n",
    "        [R_max]*n_states\n",
    "    )  # Constraint values _<=this \n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(result.x!=0).count(True)  # States better than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rev = {i:board for board,i in index_states.items()} # Reverse index state(int) -> board(tuple)\n",
    "R = {rev[i]:v for i,v in enumerate(result.x) if v!=0}  # State(board) -> Reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note*: if there are same number of 1s and 2s, the last move was made by player 2, it is a desirable state for them. \n",
    "Otherwise, more 1s than 2s, player 1 was the last to move, they try to reach this state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If player 1 reaches this state, they will win."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(R.get( (0,1,2,1,1,2,2,0,1), 0))  # 1 Will win for sure!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(R.get( (0, 1, 2, 0, 1, 2, 2, 1, 1), 0))  # 1 is winner!\n",
    "print(R.get( (1, 0, 0, 1, 2, 1, 1, 2, 2), 0))  # 1 is winner!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Symmetric states should have the same reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(R.get((2, 0, 0, 0, 0, 0, 1, 0, 1),0))\n",
    "print(R.get((2, 0, 1, 0, 0, 0, 0, 0, 1),0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But other non-optimal shouldn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(R.get((2, 0, 0, 0, 1, 0, 0, 0, 1),0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serialize and save the rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"rewards.pickle\", \"+wb\") as fd:\n",
    "    pickle.dump( R, fd )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "\n",
    "with open(\"rewards.pickle\", \"rb\") as fd:\n",
    "    Q = pickle.load( fd )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "* [Tutorial](https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html)\n",
    "* [Optimization library](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html)\n",
    "* [Simplex doc](https://docs.scipy.org/doc/scipy/reference/optimize.linprog-simplex.html)\n",
    "* [Linear programming implementation](https://github.com/yrlu/irl-imitation/blob/master/lp_irl.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
