{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Tests on Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "In this notebook I am going to test wich strategy is better to train a RL agent in a very-limited environment such as the tic-tac-toe game.\n",
    "\n",
    "\n",
    "## RL algorithm\n",
    "The implemented algorithm is the [Q-Learning algorithm](https://en.wikipedia.org/wiki/Q-learning#Algorithm), that is able to find the optimal policy $\\pi^*$.\n",
    "\n",
    "$Q$ is a fuction that given a state and an action, returns a number that means the *quality*, so the best move for a state is the one that maximizes the expected value of the total reward over all successive steps. In this case, winning the game.\n",
    "\n",
    "The easiest way to implement the function $Q$ is as a matrix ($State \\times Action$). And if the transition probability matrix is not known, then we have to sample from the environment by making the agent to play. This way is posible to calculate $Q$ iteratively.\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{new} (s_{t},a_{t}) \\leftarrow (1-\\alpha) \\cdot \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot  \\overbrace{\\bigg( \\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}} \\bigg) }^{\\text{learned value}}\n",
    "\\end{equation*}\n",
    "\n",
    "Where:\n",
    "* $r_{t}$ is the reward observed for the current state * $s_t$\n",
    "* $\\alpha \\in [0,1]$ is the learning rate, which represents the importance between previous experiences and the current one.\n",
    "* $\\gamma \\in [0,1]$ is the discount factor, which represents the difference in importance between future rewards and present rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from players.minimax import Minimax\n",
    "from players.qlearner import QLearner\n",
    "from players.random import Random\n",
    "from utils import train_player_seconds, test_players, train_player_games"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Teachers & Learners\n",
    "\n",
    "### Random teacher\n",
    "The random teacher chooses a random move each turn. It does not aim to win or lose but it plays very fast.\n",
    "\n",
    "### Minimax teacher\n",
    "The minimax teacher aims to win. This agent is optimal in the sense that it can only win or draw a game. It is slow.\n",
    "\n",
    "### RL teacher\n",
    "What if I make a RL agent to play against itself? Will it learn? On each game it learns to play as player 1 and player 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "teachers = {\n",
    "    'random': Random(2),\n",
    "    'minimax': Minimax(2)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners_seconds = {\n",
    "    'random': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': teachers['random'],\n",
    "        'results': {},\n",
    "        'train_func': lambda board,learner,teacher: learner._train_1_game(0.1, lambda x: 1-(x/10), board, teacher)\n",
    "    },\n",
    "    'minimax': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': teachers['minimax'],\n",
    "        'results': {},\n",
    "        'train_func': lambda board,learner,teacher: learner._train_1_game(0.1, lambda x: 1-(x/10), board, teacher)\n",
    "    },\n",
    "    'rl': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': None,  # Itself, referenced in future\n",
    "        'results': {},\n",
    "        'train_func': lambda board, learner, teacher: learner._autotrain_1_game(0.1, lambda x: 1-(x/10), board)\n",
    "    },\n",
    "}\n",
    "\n",
    "learners_seconds['rl']['teacher'] = learners_seconds['rl']['agent']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments (with time limitation)\n",
    "I am going to train three RL agents with the Q-learning algorithm. To make the experiment fair, I will limit the resources the agents have by setting a fixed trainning time. This way, if an agent has a better but costly in (CPU operations) teacher, will play less games than other with a worse but faster teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SECONDS = 60  # Time for training (sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rl (random) for 60 seconds\n",
      "\t 183370 games played\n",
      "\n",
      "Training rl (rl) for 60 seconds\n",
      "\t 149260 games played\n",
      "\n",
      "Training rl (minimax) for 60 seconds\n",
      "\t 278 games played\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rl_name, rl in learners_seconds.items():\n",
    "    rl['seconds'] = TRAIN_SECONDS\n",
    "    print(\"Training rl ({}) for {} seconds\".format(rl_name, TRAIN_SECONDS))\n",
    "    rl['games'] = train_player_seconds(\n",
    "        learner=rl['agent'],\n",
    "        teacher=rl['teacher'],\n",
    "        train_func=rl['train_func'],\n",
    "        seconds=TRAIN_SECONDS\n",
    "    )\n",
    "    print(\"\\t {} games played\\n\".format(rl['games']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metrics\n",
    "To measure the actual performance of each agent, I will make them play against several opponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_N_GAMES = 100  # Games to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal (minimax)\n",
    "If the agent is good enough, when faced against this oponent, no one will win and all the games will be draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing rl (random) VS minimax (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 21, 'draws': 79, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 49, 'draws': 51, 'wins': 0}\n",
      "\n",
      "\n",
      "Testing rl (rl) VS minimax (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 0, 'draws': 100, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 93, 'draws': 7, 'wins': 0}\n",
      "\n",
      "\n",
      "Testing rl (minimax) VS minimax (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 65, 'draws': 35, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 96, 'draws': 4, 'wins': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rl_name, rl in learners_seconds.items():\n",
    "    print(\"Testing rl ({}) VS minimax ({} games)\".format(rl_name, TEST_N_GAMES))\n",
    "    print(\"\\tAs player 1\")\n",
    "    results = test_players(rl['agent'], teachers['minimax'], TEST_N_GAMES)\n",
    "    rl['results']['vs_minimax (p1)'] = {\n",
    "        'wins': results[1],\n",
    "        'loses':  results[2],\n",
    "        'draws':  results[-1]\n",
    "    }\n",
    "    print(rl['results']['vs_minimax (p1)'])\n",
    "    print(\"\\tAs player 2\")\n",
    "    results = test_players(teachers['minimax'], rl['agent'], TEST_N_GAMES)\n",
    "    rl['results']['vs_minimax (p2)'] = {\n",
    "        'wins': results[2],\n",
    "        'loses':  results[1],\n",
    "        'draws':  results[-1]\n",
    "    }\n",
    "    print(rl['results']['vs_minimax (p2)'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Against other RL agent\n",
    "However, what the agent has learned could be a non-optimal policy and this way I could choose which one is the best. There can only be one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing rl (random) VS rl(random) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 100, 'draws': 0, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 0, 'draws': 0, 'wins': 100}\n",
      "\n",
      "\n",
      "Testing rl (random) VS rl(rl) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 0, 'draws': 9, 'wins': 91}\n",
      "\tAs player 2\n",
      "{'loses': 0, 'draws': 49, 'wins': 51}\n",
      "\n",
      "\n",
      "Testing rl (random) VS rl(minimax) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 6, 'draws': 9, 'wins': 85}\n",
      "\tAs player 2\n",
      "{'loses': 10, 'draws': 16, 'wins': 74}\n",
      "\n",
      "\n",
      "Testing rl (rl) VS rl(rl) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 100, 'draws': 0, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 0, 'draws': 0, 'wins': 100}\n",
      "\n",
      "\n",
      "Testing rl (rl) VS rl(minimax) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 9, 'draws': 9, 'wins': 82}\n",
      "\tAs player 2\n",
      "{'loses': 63, 'draws': 12, 'wins': 25}\n",
      "\n",
      "\n",
      "Testing rl (minimax) VS rl(minimax) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 100, 'draws': 0, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 0, 'draws': 0, 'wins': 100}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (rl1_name, rl1), (rl2_name,rl2) in itertools.combinations_with_replacement(learners_seconds.items(),2):\n",
    "    print(\"Testing rl ({}) VS rl({}) ({} games)\".format(rl1_name, rl2_name, TEST_N_GAMES))\n",
    "    print(\"\\tAs player 1\")\n",
    "    results = test_players(rl1['agent'], rl2['agent'], TEST_N_GAMES)\n",
    "    rl['results']['vs_{} (p1)'.format(rl2_name)] = {\n",
    "        'wins': results[1],\n",
    "        'loses':  results[2],\n",
    "        'draws':  results[-1]\n",
    "    }\n",
    "    print(rl['results']['vs_{} (p1)'.format(rl2_name)])\n",
    "    print(\"\\tAs player 2\")\n",
    "    results = test_players(rl2['agent'], rl1['agent'], TEST_N_GAMES)\n",
    "    rl['results']['vs_{} (p2)'.format(rl2_name)] = {\n",
    "        'wins': results[2],\n",
    "        'loses':  results[1],\n",
    "        'draws':  results[-1]\n",
    "    }\n",
    "    print(rl['results']['vs_{} (p2)'.format(rl2_name)])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of scope\n",
    "This notebook does not include how to find the best hyperparameters, it's just a glimpse on how to explore the search space and proving that in order to learn how to win, the agent must win during the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "\n",
    "## Experiments (games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_GAMES = 2000  # Number of games for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "learners_games = {\n",
    "    'random': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': teachers['random'],\n",
    "        'results': {},\n",
    "        'train_func': lambda board,learner,teacher: learner._train_1_game(0.1, lambda x: 1-(x/10), board, teacher)\n",
    "    },\n",
    "    'minimax': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': teachers['minimax'],\n",
    "        'results': {},\n",
    "        'train_func': lambda board,learner,teacher: learner._train_1_game(0.1, lambda x: 1-(x/10), board, teacher)\n",
    "    },\n",
    "    'rl': {\n",
    "        'agent': QLearner(1),\n",
    "        'teacher': None,  # Itself, referenced in future\n",
    "        'results': {},\n",
    "        'train_func': lambda board, learner, teacher: learner._autotrain_1_game(0.1, lambda x: 1-(x/10), board)\n",
    "    },\n",
    "}\n",
    "\n",
    "learners_games['rl']['teacher'] = learners_seconds['rl']['agent']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rl (random) in 2000 games\n",
      "\t 1.3201005458831787 games played\n",
      "\n",
      "Training rl (rl) in 2000 games\n",
      "\t 1.52003812789917 games played\n",
      "\n",
      "Training rl (minimax) in 2000 games\n",
      "\t 774.6538817882538 games played\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rl_name, rl in learners_games.items():\n",
    "    rl['games'] = TRAIN_GAMES\n",
    "    print(\"Training rl ({}) in {} games\".format(rl_name, TRAIN_GAMES))\n",
    "    rl['games'] = train_player_games(\n",
    "        learner=rl['agent'],\n",
    "        teacher=rl['teacher'],\n",
    "        train_func=rl['train_func'],\n",
    "        games=TRAIN_GAMES\n",
    "    )\n",
    "    print(\"\\t {} games played\\n\".format(rl['games']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_N_GAMES = 100  # Games to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Against Minimax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing rl (random) VS minimax (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 100, 'draws': 0, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 81, 'draws': 19, 'wins': 0}\n",
      "\n",
      "\n",
      "Testing rl (rl) VS minimax (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 46, 'draws': 54, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 100, 'draws': 0, 'wins': 0}\n",
      "\n",
      "\n",
      "Testing rl (minimax) VS minimax (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 0, 'draws': 100, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 56, 'draws': 44, 'wins': 0}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for rl_name, rl in learners_games.items():\n",
    "    print(\"Testing rl ({}) VS minimax ({} games)\".format(rl_name, TEST_N_GAMES))\n",
    "    print(\"\\tAs player 1\")\n",
    "    results = test_players(rl['agent'], teachers['minimax'], TEST_N_GAMES)\n",
    "    rl['results']['vs_minimax (p1)'] = {\n",
    "        'wins': results[1],\n",
    "        'loses':  results[2],\n",
    "        'draws':  results[-1]\n",
    "    }\n",
    "    print(rl['results']['vs_minimax (p1)'])\n",
    "    print(\"\\tAs player 2\")\n",
    "    results = test_players(teachers['minimax'], rl['agent'], TEST_N_GAMES)\n",
    "    rl['results']['vs_minimax (p2)'] = {\n",
    "        'wins': results[2],\n",
    "        'loses':  results[1],\n",
    "        'draws':  results[-1]\n",
    "    }\n",
    "    print(rl['results']['vs_minimax (p2)'])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Against other RL agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing rl (random) VS rl(random) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 100, 'draws': 0, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 0, 'draws': 0, 'wins': 100}\n",
      "\n",
      "\n",
      "Testing rl (random) VS rl(rl) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 0, 'draws': 46, 'wins': 54}\n",
      "\tAs player 2\n",
      "{'loses': 100, 'draws': 0, 'wins': 0}\n",
      "\n",
      "\n",
      "Testing rl (random) VS rl(minimax) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 0, 'draws': 0, 'wins': 100}\n",
      "\tAs player 2\n",
      "{'loses': 0, 'draws': 100, 'wins': 0}\n",
      "\n",
      "\n",
      "Testing rl (rl) VS rl(rl) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 100, 'draws': 0, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 0, 'draws': 0, 'wins': 100}\n",
      "\n",
      "\n",
      "Testing rl (rl) VS rl(minimax) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 0, 'draws': 15, 'wins': 85}\n",
      "\tAs player 2\n",
      "{'loses': 52, 'draws': 12, 'wins': 36}\n",
      "\n",
      "\n",
      "Testing rl (minimax) VS rl(minimax) (100 games)\n",
      "\tAs player 1\n",
      "{'loses': 100, 'draws': 0, 'wins': 0}\n",
      "\tAs player 2\n",
      "{'loses': 0, 'draws': 0, 'wins': 100}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for (rl1_name, rl1), (rl2_name,rl2) in itertools.combinations_with_replacement(learners_games.items(),2):\n",
    "    print(\"Testing rl ({}) VS rl({}) ({} games)\".format(rl1_name, rl2_name, TEST_N_GAMES))\n",
    "    print(\"\\tAs player 1\")\n",
    "    results = test_players(rl1['agent'], rl2['agent'], TEST_N_GAMES)\n",
    "    rl['results']['vs_{} (p1)'.format(rl2_name)] = {\n",
    "        'wins': results[1],\n",
    "        'loses':  results[2],\n",
    "        'draws':  results[-1]\n",
    "    }\n",
    "    print(rl['results']['vs_{} (p1)'.format(rl2_name)])\n",
    "    print(\"\\tAs player 2\")\n",
    "    results = test_players(rl2['agent'], rl1['agent'], TEST_N_GAMES)\n",
    "    rl['results']['vs_{} (p2)'.format(rl2_name)] = {\n",
    "        'wins': results[2],\n",
    "        'loses':  results[1],\n",
    "        'draws':  results[-1]\n",
    "    }\n",
    "    print(rl['results']['vs_{} (p2)'.format(rl2_name)])\n",
    "    print('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
