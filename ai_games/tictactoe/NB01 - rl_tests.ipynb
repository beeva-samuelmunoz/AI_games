{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Tests on Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "In this notebook I am going to test wich strategy is better to train a RL agent in a very-limited environment such as the tic-tac-toe game.\n",
    "\n",
    "\n",
    "## RL algorithm\n",
    "The implemented algorithm is the [Q-Learning algorithm](https://en.wikipedia.org/wiki/Q-learning#Algorithm), that is able to find the optimal policy $\\pi^*$.\n",
    "\n",
    "$Q$ is a fuction that given a state and an action, returns a number that means the *quality*, so the best move for a state is the one that maximizes the expected value of the total reward over all successive steps. In this case, winning the game.\n",
    "\n",
    "The easiest way to implement the function $Q$ is as a matrix ($State \\times Action$). And if the transition probability matrix is not known, then we have to sample from the environment by making the agent to play. This way is posible to calculate $Q$ iteratively.\n",
    "\n",
    "\\begin{equation*}\n",
    "Q^{new} (s_{t},a_{t}) \\leftarrow (1-\\alpha) \\cdot \\underbrace{Q(s_{t},a_{t})}_{\\text{old value}} + \\underbrace{\\alpha}_{\\text{learning rate}} \\cdot  \\overbrace{\\bigg( \\underbrace{r_{t}}_{\\text{reward}} + \\underbrace{\\gamma}_{\\text{discount factor}} \\cdot \\underbrace{\\max_{a}Q(s_{t+1}, a)}_{\\text{estimate of optimal future value}} \\bigg) }^{\\text{learned value}}\n",
    "\\end{equation*}\n",
    "\n",
    "Where:\n",
    "* $r_{t}$ is the reward observed for the current state * $s_t$\n",
    "* $\\alpha \\in [0,1]$ is the learning rate, which represents the importance between previous experiences and the current one.\n",
    "* $\\gamma \\in [0,1]$ is the discount factor, which represents the difference in importance between future rewards and present rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from players.minimax import Minimax\n",
    "from players.qlearner import QLearner\n",
    "from players.random import Random\n",
    "from utils_train import train_player, test_players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiments\n",
    "I am going to train three RL agents with the Q-learning algorithm. To make the experiment fair, I will limit the resources the agents have by setting a fixed trainning time. This way, if an agent has a better but costly in (CPU operations) teacher, will play less games than other with a worse but faster teacher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SECONDS = 60  # Time for training (sg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 Random teacher\n",
    "The random teacher chooses a random move each turn. It does not aim to win or lose but it plays very fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_rl_rand = QLearner(1)\n",
    "p_rand = Random(2)\n",
    "\n",
    "print(\"Training for {} seconds\".format(TRAIN_SECONDS))\n",
    "games_rand = train_player(\n",
    "    p_train=p_rl_rand,\n",
    "    p_opponent=p_rand,\n",
    "    p_train_func=lambda board: p_rl_rand._train_1_game(0.1, lambda x: 1-(x/10), board, p_rand),\n",
    "    seconds=TRAIN_SECONDS\n",
    ")\n",
    "print(\"Games played: {}\".format(games_rand))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Minimax teacher\n",
    "The minimax teacher aims to win. This agent is optimal in the sense that it can only win or draw a game. It is slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_rl_minimax = QLearner(1)\n",
    "p_minimax = Minimax(2)\n",
    "\n",
    "print(\"Training for {} seconds\".format(TRAIN_SECONDS))\n",
    "games_minimax = train_player(\n",
    "    p_train=p_rl_minimax,\n",
    "    p_opponent=p_minimax,\n",
    "    p_train_func=lambda board: p_rl_minimax._train_1_game(0.1, lambda x: 1-(x/10), board, p_minimax),\n",
    "    seconds=TRAIN_SECONDS\n",
    ")\n",
    "print(\"Games played: {}\".format(games_minimax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 RL teacher\n",
    "What if I make a RL agent to play against itself? Will it learn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_rl_rl = QLearner(1)\n",
    "\n",
    "print(\"Training for {} seconds\".format(TRAIN_SECONDS))\n",
    "\n",
    "games_minimax = train_player(\n",
    "    p_train=p_rl_rl,\n",
    "    p_opponent=p_rl_rl,\n",
    "    p_train_func=lambda board: p_rl_rl._autotrain_1_game(0.1, lambda x: 1-(x/10), board),\n",
    "    seconds=TRAIN_SECONDS\n",
    ")\n",
    "print(\"Games played: {}\".format(games_minimax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Metrics\n",
    "To measure the actual performance of each agent, I will make them play against several opponents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_N_GAMES = 100  # Games to test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal (minimax)\n",
    "If the agent is good enough, when faced against this oponent, no one will win and all the games will be draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing RL-RAND VS MINIMAX ({} games)\".format(TEST_N_GAMES))\n",
    "print(\"As player 1\")\n",
    "print(test_players(p_rl_rand, p_minimax, TEST_N_GAMES))\n",
    "print(\"As player 2\")\n",
    "print(test_players(p_minimax, p_rl_rand, TEST_N_GAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing RL-MINIMAX VS MINIMAX ({} games)\".format(TEST_N_GAMES))\n",
    "print(\"As player 1\")\n",
    "print(test_players(p_rl_minimax, p_minimax, TEST_N_GAMES))\n",
    "print(\"As player 2\")\n",
    "print(test_players(p_minimax, p_rl_minimax, TEST_N_GAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing RL-RL VS MINIMAX ({} games)\".format(TEST_N_GAMES))\n",
    "print(\"As player 1\")\n",
    "print(test_players(p_rl_rl, p_minimax, TEST_N_GAMES))\n",
    "print(\"As player 2\")\n",
    "print(test_players(p_minimax, p_rl_rl, TEST_N_GAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Against other RL agent\n",
    "However, what the agent has learned could be a non-optimal policy and this way I could choose which one is the best. There can only be one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing RL-RAND VS RL-MINIMAX ({} games)\".format(TEST_N_GAMES))\n",
    "print(\"As player 1\")\n",
    "print(test_players(p_rl_rand, p_rl_minimax, TEST_N_GAMES))\n",
    "print(\"As player 2\")\n",
    "print(test_players(p_rl_minimax, p_rl_rand, TEST_N_GAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing RL-RAND VS RL-RL ({} games)\".format(TEST_N_GAMES))\n",
    "print(\"As player 1\")\n",
    "print(test_players(p_rl_rand, p_rl_rl, TEST_N_GAMES))\n",
    "print(\"As player 2\")\n",
    "print(test_players(p_rl_rl, p_rl_rand, TEST_N_GAMES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing RL-MINIMAX VS RL-RL ({} games)\".format(TEST_N_GAMES))\n",
    "print(\"As player 1\")\n",
    "print(test_players(p_rl_minimax, p_rl_rl, TEST_N_GAMES))\n",
    "print(\"As player 2\")\n",
    "print(test_players(p_rl_rl, p_rl_minimax, TEST_N_GAMES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of scope\n",
    "This notebook does not include how to find the best hyperparameters, it's just a glimpse on how to explore the search space and proving that in order to learn how to win, the agent must win during the training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
